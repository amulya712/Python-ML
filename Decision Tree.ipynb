{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "A decision tree is essentially a series of if-then statements, that, when applied to a record in a data set, results in the classification of that record. Therefore, once you've created your decision tree, you will be able to run a data set through the program and get a classification for each individual record within the data set. What this means to you, as a manufacturer of quality widgets, is that the program you create from this article will be able to predict the likelihood of each user, within a data set, purchasing your finely crafted product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is a type of supervised learning algorithm (having a pre-defined target variable) that is mostly used in classification problems. It works for both categorical and continuous input and output variables. In this technique, we split the population or sample into two or more homogeneous sets (or sub-populations) based on most significant splitter / differentiator in input variables.\n",
    "\n",
    "<img src=\"images/2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How decision tree works\n",
    "\n",
    "\n",
    "The understanding level of Decision Trees algorithm is so easy compared with other classification algorithms. The decision tree algorithm tries to solve the problem, by using tree representation. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.\n",
    "\n",
    "## Decision Tree Algorithm Pseudocode\n",
    "\n",
    "   * Place the best attribute of the dataset at the root of the tree.\n",
    "   * Split the training set into subsets. Subsets should be made in such a way that each subset contains data with the same value for an attribute.\n",
    "   * Repeat step 1 and step 2 on each subset until you find leaf nodes in all the branches of the tree.\n",
    " \n",
    "\n",
    "### Decision Tree classifier\n",
    "In decision trees, for predicting a class label for a record we start from the root of the tree. We compare the values of the root attribute with record’s attribute. On the basis of comparison, we follow the branch corresponding to that value and jump to the next node.\n",
    "\n",
    "We continue comparing our record’s attribute values with other internal nodes of the tree until we reach a leaf node with predicted class value. As we know how the modeled decision tree can be used to predict the target class or the value. Now let’s understanding how we can create the decision tree model.\n",
    "\n",
    "### Assumptions while creating Decision Tree\n",
    "\n",
    "The below are the some of the assumptions we make while using Decision tree:\n",
    "\n",
    "   * At the beginning, the whole training set is considered as the root.\n",
    "   * Feature values are preferred to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "   * Records are distributed recursively on the basis of attribute values.\n",
    "   * Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split nodes\n",
    "\n",
    "There are few algorithms to find optimum split. Let's look at the following to understand the mathematics behind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "An alternative splitting criterion for decision tree learning algorithms is *information gain*. It measures how well a particular attribute distinguishes among different target classifications. Information gain is measured in terms of the expected reduction in the entropy or impurity of the data. The entropy of a set of probabilities is:\n",
    "\n",
    "$$H(p) = -\\sum_i p_i log_2(p_i)$$\n",
    "\n",
    "If we have a set of binary responses from some variable, all of which are positive/true/1, then knowing the values of the variable does not hold any predictive value for us, since all the outcomes are positive. Hence, the entropy is zero:\n",
    "\n",
    "<img src=\"images/ent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy calculation tells us how much additional information we would obtain with knowledge of the variable.\n",
    "\n",
    "So, if we have a set of candidate covariates from which to choose as a node in a decision tree, we should choose the one that gives us the most information about the response variable (*i.e.* the one with the highest entropy).\n",
    "\n",
    "### Misclassification Rate\n",
    "\n",
    "Alternatively, we can use the misclassification rate:\n",
    "\n",
    "$$C(j,t) = \\frac{1}{n_{jt}} \\sum_{y_i: x_{ij} \\gt t} I(y_i \\ne \\hat{y})$$\n",
    "\n",
    "where $\\hat{y}$ is the most probable class label and $n_{ij}$ is the number of observations in the data subset obtained from splitting via $j,t$.\n",
    "\n",
    "### Gini index\n",
    "\n",
    "The Gini index is simply the expected error rate:\n",
    "\n",
    "$$C(j,t) = \\sum_{k=1}^K \\hat{\\pi}_{jt}[k] (1 - \\hat{\\pi}_{jt}[k]) = 1 - \\sum_{k=1}^K \\hat{\\pi}_{jt}[k]^2$$\n",
    "\n",
    "where $\\hat{\\pi}_{jt}[k]$ is the probability of an observation being correctly classified as class $k$ for the data subset obtained from splitting via $j,t$ (hence, $(1 - \\hat{\\pi}_{jt}[k])$ is the misclassification probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3\n",
    "\n",
    "A given cost function can be used to construct a decision tree via one of several algorithms. The Iterative Dichotomiser 3 (ID3) is on such algorithm, which uses entropy, and a related concept, *information gain*, to choose features and partitions at each classification step in the tree.\n",
    "\n",
    "Information gain is the difference between the current entropy of a system and the entropy measured after a feature is chosen. If $S$ is a set of examples and $X$ is a possible feature on which to partition the examples, then:\n",
    "\n",
    "$$G(S,X) = \\text{Entropy}(S) - \\sum_{x \\in X} \\frac{\\#(S_x)}{\\#(S)} \\text{Entropy}(S_x)$$\n",
    "\n",
    "where $\\#$ is the count function and $x$ is a particular value of $X$.\n",
    "\n",
    "Let's say $S$ is a set of survival events, $S = \\{s_1=survived, s_2=died, s_3=died, s_4=died\\}$ and a particular variable $X$ can have values $\\{x_1, x_2, x_3\\}$. To perform a sample calculation of information gain, we will say that:\n",
    "\n",
    "* $X(s_1) = x_2$\n",
    "* $X(s_2) = x_2$\n",
    "* $X(s_3) = x_3$\n",
    "* $X(s_4) = x_1$\n",
    "\n",
    "The current entropy of this state is:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Entropy}(S) &= -p^{(+)} \\log_2(p^{(+)}) - p^{(-)} \\log_2(p^{(-)}) \\\\\n",
    "&= -0.25 \\log_2(0.25) - 0.75 \\log_2(0.75) \\\\\n",
    "&= 0.5 + 0.311 = 0.811\n",
    "\\end{align}$$\n",
    "\n",
    "Now, we need to compute the information after selecting variable $X$, which is the sum of three terms:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\#(S_{x1})}{\\#(S)} \\text{Entropy}(S) &= 0.25 (-0 \\log_2(0) - 1 \\log_2(1)) = 0\\\\\n",
    "\\frac{\\#(S_{x2})}{\\#(S)} \\text{Entropy}(S) &= 0.5 (-0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 0.5\\\\\n",
    "\\frac{\\#(S_{x3})}{\\#(S)} \\text{Entropy}(S) &= 0.25 (-0 \\log_2(0) - 1 \\log_2 1) = 0\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, the information gain is:\n",
    "\n",
    "$$G(S,X) = 0.811 - (0 + 0.5 + 0) = 0.311$$\n",
    "\n",
    "\n",
    "<img src=\"images/com.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np,pandas as pd,matplotlib.pyplot as plt\n",
    "from sklearn import tree,metrics,model_selection,preprocessing\n",
    "from IPython.display import Image,display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('iris.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select features\n",
    "y=df['species']\n",
    "X=df[['petal_length','petal_width']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data randomly into 70% training and 30% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and make predictions\n",
    "\n",
    "Note we didn't have to standardize the data to use a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the decision tree\n",
    "from sklearn import tree\n",
    "dtree=tree.DecisionTreeClassifier(criterion='entropy',max_depth=3,random_state=0)\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-virginica', 'Iris-versicolor', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-setosa', 'Iris-virginica', 'Iris-setosa',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-setosa', 'Iris-virginica',\n",
       "       'Iris-versicolor', 'Iris-setosa', 'Iris-virginica',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-virginica', 'Iris-setosa', 'Iris-virginica', 'Iris-setosa',\n",
       "       'Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the model to make predictions with the test data\n",
    "#print(X_test)\n",
    "y_pred=dtree.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Evaluate the model's performance\n",
    "\n",
    "Including the tree's axis-parallel decision boundaries and how the tree splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how did our model perform?\n",
    "y_pred1=dtree.predict([[1.4,0.2]])\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassified samples:1\n",
      "Accuracy:0.98\n"
     ]
    }
   ],
   "source": [
    "count_missclassified=(y_test!=y_pred).sum()\n",
    "print('Missclassified samples:{}'.format(count_missclassified))\n",
    "accuracy=metrics.accuracy_score(y_test,y_pred)\n",
    "print('Accuracy:{:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize how the tree splits using [GraphViz](http://www.graphviz.org/) (make sure you install it first):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Cross Validation is a technique which involves reserving a particular sample of a data set on which you do not train the model. Later, you test the model on this sample before finalizing the model.\n",
    "\n",
    "Here are the steps involved in cross validation:\n",
    "\n",
    "   * You reserve a sample data set.\n",
    "   * Train the model using the remaining part of the data set.\n",
    "   * Use the reserve sample of the data set test (validation) set. This will help you to know the effectiveness of model performance. It your model delivers a positive result on validation data, go ahead with current model. It rocks!\n",
    "   \n",
    "   \n",
    "<img src=\"images/crossval.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Validation and Cross Validation\n",
    "\n",
    "When creating a predictive model, we'd like to get an accurate sense of its ability to generalize to unseen data before actually going out and using it on unseen data. As we saw earlier, generating predictions on the training data itself to check the model's accuracy does not work very well: a complex model may fit the training data extremely closely but fail to generalize to new, unseen data. We can get a better sense of a model's expected performance on unseen data by setting a portion of our training data aside when creating a model, and then using that set aside data to evaluate the model's performance. This technique of setting aside some of the training data to assess a model's ability to generalize is known as validation.\n",
    "\n",
    "Holdout validation and cross validation are two common methods for assessing a model before using it on test data. Holdout validation involves splitting the training data into two parts, a training set and a validation set, building a model with the training set and then assessing performance with the validation set. In theory, model performance on the hold-out validation set should roughly mirror the performance you'd expect to see on unseen test data. In practice, holdout validation is fast and it can work well, especially on large data sets, but it has some pitfalls.\n",
    "\n",
    "Reserving a portion of the training data for a holdout set means you aren't using all the data at your disposal to build your model in the validation phase. This can lead to suboptimal performance, especially in situations where you don't have much data to work with. In addition, if you use the same holdout validation set to assess too many different models, you may end up finding a model that fits the validation set well due to chance that won't necessarily generalize well to unseen data. Despite these shortcomings, it is worth learning how to use a holdout validation set in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After creating a cross validation object, you can loop over each fold and train and evaluate a your model on each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per fold:\n",
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.93333333 0.93333333 1.         1.        ]\n",
      "Average accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "scores=cross_val_score(estimator=dtree,\n",
    "                      X=X,\n",
    "                      y=y,\n",
    "                      scoring=\"accuracy\",\n",
    "                      cv=10)\n",
    "print(\"Accuracy per fold:\")\n",
    "print(scores)\n",
    "print(\"Average accuracy:\",scores.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Overfitting is a practical problem while building a decision tree model. The model is having an issue of overfitting is considered when the algorithm continues to go deeper and deeper in the to reduce the training set error but results with an increased test set error i.e, Accuracy of prediction for our model goes down. It generally happens when it builds many branches due to outliers and irregularities in data.\n",
    "\n",
    "#### Two approaches which we can use to avoid overfitting are:\n",
    "\n",
    "   * Pre-Pruning\n",
    "   * Post-Pruning\n",
    "\n",
    "\n",
    "##### Pre-Pruning\n",
    "\n",
    "In pre-pruning, it stops the tree construction bit early. It is preferred not to split a node if its goodness measure is below a threshold value. But it’s difficult to choose an appropriate stopping point.\n",
    "\n",
    "##### Post-Pruning\n",
    "\n",
    "In post-pruning first, it goes deeper and deeper in the tree to build a complete tree. If the tree shows the overfitting problem then pruning is done as a post-pruning step. We use a cross-validation data to check the effect of our pruning. Using cross-validation data, it tests whether expanding a node will make an improvement or not.\n",
    "\n",
    "If it shows an improvement, then we can continue by expanding that node. But if it shows a reduction in accuracy then it should not be expanded i.e, the node should be converted to a leaf node.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/pruning.png\">\n",
    "\n",
    "### Decision Tree Algorithm Advantages and Disadvantages\n",
    "\n",
    "##### Advantages:\n",
    "\n",
    "   * Decision Trees are easy to explain. It results in a set of rules.\n",
    "   * It follows the same approach as humans generally follow while making decisions.\n",
    "   * Interpretation of a complex Decision Tree model can be simplified by its visualizations. Even a naive person can understand logic.\n",
    "   * The Number of hyper-parameters to be tuned is almost null.\n",
    "\n",
    "\n",
    "##### Disadvantages:\n",
    "\n",
    "   * There is a high probability of overfitting in Decision Tree.\n",
    "   * Generally, it gives low prediction accuracy for a dataset as compared to other machine learning algorithms.\n",
    "   * Information gain in a decision tree with categorical variables gives a biased response for attributes with greater no. of categories.\n",
    "   * Calculations can become complex when there are many class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
